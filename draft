import optuna
import os
loaded_study = optuna.load_study(study_name="TEST_PRUNING", storage="sqlite:///example.db")
#importances = optuna.importance.get_param_importances(loaded_study)
fig = optuna.visualization.plot_param_importances(loaded_study)


cwd = os.getcwd()
fig.write_image(cwd+"/hyper_importance.png")


Determining the best hyperparameters (the parameters used
by backpropagation) is a challenging task, especially as the
heuristics for deep learning grow more complicated. The
parameters are interrelated and there can be a fairly large
number of them. For example, in this work there are 11
different hyperparameters that can be modified to effect the
training algorithm (see Section VII). How to best determine
these hyperparameters is still an open problem, and can be
very time consuming

running_loss += loss.item() * inputs.size(0)
running_corrects += torch.sum(preds == labels.data)

epoch_loss = running_loss / len(dataloader[phase].dataset)


def get_scores(output,uncertainty_types,evi_fun):
    # output: the output of a model
    scores = dict.fromkeys(uncertainty_types, 0)
    s = ScoreCal(output,evi_fun)
    cal_funcs = ['-s.cal_max_prob_SL()','s.cal_entropy_SL()','s.cal_vacuity()','s.cal_dissonance()']
    # Note that the -max_prob as the uncertainty measure

    for i,un in enumerate(uncertainty_types[:-1]):
        cal_func = cal_funcs[i]
        scores[un] = eval(cal_func)

    scores[uncertainty_types[-1]] = np.max(np.concatenate((scores['vacuity'],scores['dissonance']),axis=1),axis=1,keepdims = True)
    pred_labels = np.argmax(s.prob_SL, axis=1)
    return scores,pred_labels

def update_result_acc(df, temp_data, pred, true):
    # pred: prediction Results (not labels)
    # true: Ground truth labels
    _,true_class_n = np.unique(true,return_counts=True)
    for class_index,class_n in enumerate(true_class_n):
        true_each_class = true == class_index
        pred_result_each_class = np.logical_and(pred,true_each_class)
        temp_data['recall'] = np.sum(pred_result_each_class)/class_n
        temp_data['gesture'] = class_index + 1
        df = df.append([temp_data])
    return df

def update_result_un(df, temp_data, labels, scores):
    # could be used for both mis detection and ood detection
    # For ood detection, temp_data contains 'unseen_gesture'
    # scores: dict: store score of each uncertainty
    for uncertainty_type, score in scores.items():
        temp = PREval(labels,score)
        temp_data['AP_nor'] = temp.AP_nor_cal()
        temp_data['uncertainty_type'] = uncertainty_type
        df = df.append([temp_data])
    return df

        device = torch.device('cpu')
        test_net = model.Net(number_of_class=class_num,dropout=.5,k_c=kernel).to(device)
        test_net.load_state_dict(torch.load(savefile,map_location=torch.device('cpu')))
        test_net.eval()

        #uncertainty_types = ['max_prob','entropy','vacuity','dissonance']
        uncertainty_types = ['max_prob','entropy','vacuity','dissonance','max(vac,diss)']
        scores = dict.fromkeys(uncertainty_types, 0)
        #scores_ood = dict.fromkeys(uncertainty_types, 0)

        # Acc (recall)
        col_name = ['sb_n','test_session','recall','gesture']
        temp_data_acc = dict.fromkeys(col_name, 1)
        df_acc = pd.DataFrame(columns = col_name)

        # Misclassification
        col_name = ['sb_n','test_session','AP_nor','uncertainty_type','skew']
        temp_data_mis = dict.fromkeys(col_name, 1)
        df_mis = pd.DataFrame(columns = col_name)


        X = torch.as_tensor(torch.from_numpy(np.array(self.X))).permute(0,1,3,2) # L*1*16*50
        Y = torch.from_numpy(np.array(self.Y, dtype=np.int64))

 data.read_data(ext_c = [test_index],sb_n = sb_num,ex_n = 1)
        X_test,Y_test = data.load_data_cnn_as_tensor()
        inputs = X_test.to(device)
        Y_test = Y_test.numpy()
        del data

        output = test_net(inputs)

        scores_in, pred_labels = get_scores(output,uncertainty_types,evi_fun)
        pred_results = pred_labels==Y_test

        ## Update acc
        df_acc = update_result_acc(df_acc, temp_data_acc, pred_results,Y_test)

        ## Update mis

        # Get label  pos: wrong predictions; neg: right predictions
        labels = np.logical_not(pred_results)
        print(labels)
        n_in = len(labels)
        temp_data_mis['skew'] = np.count_nonzero(labels)/n_in
        #print(labels.shape)
        df_mis = update_result_un(df_mis,temp_data_mis,labels,scores_in)

       # print results directly
        print('-'*20)
        print('ovarall test acc')
        print(df_acc['recall'].mean())
        print('Acc for each class')
        print(df_acc['recall'].to_string(index=False))
        print('-'*20)
        df = df_mis.groupby(['uncertainty_type'])['AP_nor'].mean()
        print('mis detection results')
        print(df)
        print(df.to_string(index=False))













\section{Robust hand gesture recognition}
%\section{New framework to solve the inter-subject variability}
This chapter describes the robustness of the sEMG based hand gesture recognition. Different from accuracy and reliability, the robustness is an important evaluation indicator especially when considering the practical performance of hand gesture recognition. Basically, we argue that the robustness of the sEMG based hand gesture classifier can be defined as the ability of against different variability dimensions. The key of designing a robust classifier is to find invariant features or design a new framework.

The current contribution of this chapter is that we proposed a framework to reduce the inter-subject variability on the sEMG-based hand gesture recognition. We found that a referencing  min-max  normalisation  approach can be used  to  re-weight  the  source  domain  sEMG data, and therefore to reduce the  domain  shift. The results are compared to the state-of-the-art transfer learning approach.

%\section{Comprehensive analysis on the robustness of sEMG-based hand gesture recognition}
